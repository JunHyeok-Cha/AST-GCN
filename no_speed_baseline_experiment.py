# baseline_experiment.py
#
# 1) X_samples.npy, Y_samples.npy ë¡œë¶€í„° DataLoader ìƒì„±
# 2) No-Graph Baseline ëª¨ë¸(MLP / LSTM) ì •ì˜
# 3) í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ë° ì„±ëŠ¥ ì¶œë ¥
#
# âš  ì „ì œ: ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸
#   - T_in, T_out
#   - X_samples.npy: (num_samples, N, T_in, F)
#   - Y_samples.npy: (num_samples, N, T_out)
#   ì´ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.

import os
from pathlib import Path
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, Subset
from typing import Tuple, List


# =========================
# 0. ê³µí†µ ì„¤ì •
# =========================

data_dir = Path("/mnt/c/Source/python/AST-GCN/res")

X_path = data_dir / "X_samples.npy"
Y_path = data_dir / "Y_samples.npy"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# =========================
# 1. Dataset ì •ì˜
# =========================

class TrafficSamplesDataset(Dataset):
    """
    X_samples, Y_samples ë¥¼ ë˜í•‘í•˜ëŠ” Dataset.
    
    X: (num_samples, N, T_in, F)
    Y: (num_samples, N, T_out)
    
    í•œ "ìƒ˜í”Œ"ì€ í•˜ë‚˜ì˜ "í•˜ë£¨ ì‹œê³„ì—´ ìœˆë„ìš° (T_in ì‹œê°„)" ì´ê³ ,
    ê·¸ ì•ˆì— Nê°œì˜ ë…¸ë“œê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆë‹¤.
    """
    def __init__(self, X: np.ndarray, Y: np.ndarray):
        assert X.shape[0] == Y.shape[0], "ìƒ˜í”Œ ê°œìˆ˜ ë¶ˆì¼ì¹˜"
        self.X = torch.from_numpy(X).float()  # (S, N, T_in, F)
        self.Y = torch.from_numpy(Y).float()  # (S, N, T_out)

    def __len__(self) -> int:
        return self.X.shape[0]

    def __getitem__(self, idx: int):
        # ë°˜í™˜ í˜•íƒœ: (N, T_in, F), (N, T_out)
        return self.X[idx], self.Y[idx]

def get_dataloaders(
    X_path: Path,
    Y_path: Path,
    batch_size: int = 4,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    seed: int = 42,
    use_speed: bool = True,      # ğŸ”¥ speed ì“¸ì§€ ë§ì§€
):
    """
    use_speed = True  â†’ ëª¨ë“  í”¼ì²˜ ì‚¬ìš© (TotalTraffic, GetOn, GetOff, RouteCount, Speed)
    use_speed = False â†’ Speed ì±„ë„ ì œê±° (ë§ˆì§€ë§‰ ì±„ë„ë§Œ ë‚ ë¦¼)
    """

    X = np.load(X_path)  # (S, N, T_in, F=5)
    Y = np.load(Y_path)  # (S, N, T_out)

    if use_speed:
        X_used = X                      # (S, N, T_in, 5)
    else:
        X_used = X[..., :-1]            # (S, N, T_in, 4)  â† Speed ì œê±°

    S, N, T_in, F = X_used.shape
    _, _, T_out = Y.shape

    print(f"[Data] use_speed={use_speed}")
    print(f"[Data] X_used: {X_used.shape}, Y: {Y.shape}")

    dataset = TrafficSamplesDataset(X_used, Y)

    # ì•„ë˜ëŠ” ê·¸ëŒ€ë¡œ
    rng = np.random.RandomState(seed)
    indices = np.arange(S)
    rng.shuffle(indices)

    n_test = int(S * test_ratio)
    n_val = int(S * val_ratio)
    n_train = S - n_val - n_test

    train_idx = indices[:n_train]
    val_idx   = indices[n_train:n_train + n_val]
    test_idx  = indices[n_train + n_val:]

    print(f"[Split] train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}")

    train_ds = Subset(dataset, train_idx)
    val_ds   = Subset(dataset, val_idx)
    test_ds  = Subset(dataset, test_idx)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)

    meta = dict(N=N, T_in=T_in, F=F, T_out=T_out)
    return train_loader, val_loader, test_loader, meta


# =========================
# 2. No-Graph Baseline ëª¨ë¸ë“¤
# =========================

class MLPBaseline(nn.Module):
    """
    ê·¸ë˜í”„ ì •ë³´ë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Baseline.
    
    - ì…ë ¥: x (B, N, T_in, F)
    - ë‚´ë¶€: ê° ë…¸ë“œ ì‹œí€€ìŠ¤ (T_in * F) ë¥¼ ë²¡í„°ë¡œ í¼ì³ì„œ
            ê³µìœ  MLP ì— ë„£ëŠ”ë‹¤ (ë…¸ë“œë³„ ë…ë¦½ ì²˜ë¦¬, íŒŒë¼ë¯¸í„° ê³µìœ ).
    - ì¶œë ¥: y_hat (B, N, T_out)
    """
    def __init__(
        self,
        T_in: int,
        F: int,
        T_out: int = 1,
        hidden_dims: List[int] = [64, 64],
        dropout: float = 0.1
    ):
        super().__init__()
        self.T_in = T_in
        self.F = F
        self.T_out = T_out

        in_dim = T_in * F
        layers = []
        prev_dim = in_dim

        for h_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.ReLU())
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            prev_dim = h_dim

        # ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ: T_out (ë³´í†µ 1ì‹œê°„ ì˜ˆì¸¡)
        layers.append(nn.Linear(prev_dim, T_out))

        self.mlp = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, N, T_in, F)
        return: (B, N, T_out)
        """
        B, N, T_in, F = x.shape
        assert T_in == self.T_in and F == self.F

        # (B, N, T_in, F) -> (B * N, T_in * F)
        x_flat = x.view(B * N, T_in * F)

        # MLP í†µê³¼: (B * N, T_out)
        y_flat = self.mlp(x_flat)

        # (B, N, T_out) í˜•íƒœë¡œ ë˜ëŒë¦¬ê¸°
        y = y_flat.view(B, N, self.T_out)
        return y


class LSTMBaseline(nn.Module):
    """
    ê·¸ë˜í”„ ì •ë³´ë¥¼ ì „í˜€ ì“°ì§€ ì•ŠëŠ” LSTM Baseline.
    
    - ì…ë ¥: x (B, N, T_in, F)
    - ë‚´ë¶€: (B * N, T_in, F) ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ í›„
            ê³µìœ  LSTM ì ìš© â†’ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… hidden ì‚¬ìš©
    - ì¶œë ¥: y_hat (B, N, T_out)
    """
    def __init__(
        self,
        F: int,
        hidden_size: int = 64,
        num_layers: int = 1,
        T_out: int = 1,
        dropout: float = 0.0,
        bidirectional: bool = False
    ):
        super().__init__()
        self.F = F
        self.T_out = T_out
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional

        self.lstm = nn.LSTM(
            input_size=F,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,   # ì…ë ¥ (B*N, T_in, F)
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=bidirectional
        )
        lstm_out_dim = hidden_size * (2 if bidirectional else 1)

        self.fc = nn.Linear(lstm_out_dim, T_out)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, N, T_in, F)
        return: (B, N, T_out)
        """
        B, N, T_in, F = x.shape
        assert F == self.F

        # (B, N, T_in, F) -> (B*N, T_in, F)
        x_seq = x.view(B * N, T_in, F)

        out, (h_n, c_n) = self.lstm(x_seq)
        # out: (B*N, T_in, hidden)
        # ì—¬ê¸°ì„œëŠ” ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ output ì‚¬ìš©
        h_last = out[:, -1, :]  # (B*N, hidden*dir)

        y_flat = self.fc(h_last)    # (B*N, T_out)
        y = y_flat.view(B, N, self.T_out)
        return y


# =========================
# 3. í•™ìŠµ / í‰ê°€ ë£¨í”„
# =========================

def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module
) -> Tuple[float, float]:
    """
    í•œ epoch ë™ì•ˆ train_loader ì— ëŒ€í•´ í•™ìŠµí•˜ê³ ,
    í‰ê·  loss / MAE ë¥¼ ë°˜í™˜.
    """
    model.train()
    total_loss = 0.0
    total_mae = 0.0
    total_count = 0

    for xb, yb in loader:
        xb = xb.to(device)  # (B, N, T_in, F)
        yb = yb.to(device)  # (B, N, T_out)

        optimizer.zero_grad()

        y_hat = model(xb)   # (B, N, T_out)
        loss = criterion(y_hat, yb)

        loss.backward()
        optimizer.step()

        # í†µê³„ ìŒ“ê¸°
        B = xb.size(0)
        total_loss += loss.item() * B

        # MAE ê³„ì‚°
        mae = (y_hat - yb).abs().mean().item()
        total_mae += mae * B
        total_count += B

    avg_loss = total_loss / total_count
    avg_mae = total_mae / total_count
    return avg_loss, avg_mae


@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module
) -> Tuple[float, float]:
    """
    Val/Test ìš© í‰ê°€ í•¨ìˆ˜.
    í‰ê·  loss / MAE ë°˜í™˜.
    """
    model.eval()
    total_loss = 0.0
    total_mae = 0.0
    total_count = 0

    for xb, yb in loader:
        xb = xb.to(device)
        yb = yb.to(device)

        y_hat = model(xb)
        loss = criterion(y_hat, yb)

        B = xb.size(0)
        total_loss += loss.item() * B

        mae = (y_hat - yb).abs().mean().item()
        total_mae += mae * B
        total_count += B

    avg_loss = total_loss / total_count
    avg_mae = total_mae / total_count
    return avg_loss, avg_mae


# =========================
# 4. ë©”ì¸: MLP / LSTM ì¤‘ í•˜ë‚˜ ê³¨ë¼ì„œ í•™ìŠµ
# =========================

def main(model_type: str = "mlp", use_speed: bool = True):
    # 1) DataLoader ì¤€ë¹„
    train_loader, val_loader, test_loader, meta = get_dataloaders(
        X_path, Y_path,
        batch_size=2,
        val_ratio=0.15,
        test_ratio=0.15,
        seed=42,
        use_speed=use_speed,        # ğŸ”¥ ì—¬ê¸°!
    )
    N, T_in, F, T_out = meta["N"], meta["T_in"], meta["F"], meta["T_out"]
    print("Meta:", meta)

    # 2) ëª¨ë¸ ì„ íƒ (ê·¸ëŒ€ë¡œ, Fë§Œ metaì—ì„œ ë°›ì•„ì„œ ì‚¬ìš©)
    if model_type == "mlp":
        model = MLPBaseline(
            T_in=T_in,
            F=F,
            T_out=T_out,
            hidden_dims=[64, 64],
            dropout=0.1,
        )
    elif model_type == "lstm":
        model = LSTMBaseline(
            F=F,
            hidden_size=64,
            num_layers=1,
            T_out=T_out,
            dropout=0.0,
            bidirectional=False,
        )
    else:
        raise ValueError("model_type ì€ 'mlp' ë˜ëŠ” 'lstm' ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    model = model.to(device)
    print(model)

    # 3) í•™ìŠµ (ê·¸ëŒ€ë¡œ)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)

    num_epochs = 100
    best_val_loss = float("inf")
    best_state = None

    for epoch in range(1, num_epochs + 1):
        train_loss, train_mae = train_one_epoch(model, train_loader, optimizer, criterion)
        val_loss, val_mae = evaluate(model, val_loader, criterion)

        print(
            f"[Epoch {epoch:03d}] "
            f"Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f} | "
            f"Val Loss: {val_loss:.4f}, MAE: {val_mae:.4f}"
        )

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = model.state_dict()

    if best_state is not None:
        model.load_state_dict(best_state)

    test_loss, test_mae = evaluate(model, test_loader, criterion)
    print(f"[Test] Loss: {test_loss:.4f}, MAE: {test_mae:.4f}")

    # 4) ì €ì¥í•  ë•Œ ì´ë¦„ìœ¼ë¡œ êµ¬ë¶„
    tag = "withspeed" if use_speed else "nospeed"
    save_path = data_dir / f"baseline_{model_type}_{tag}_best.pth"
    torch.save({
        "model_type": model_type,
        "use_speed": use_speed,
        "state_dict": model.state_dict(),
        "meta": meta,
    }, save_path)
    print("Saved best model to:", save_path)



if __name__ == "__main__":
    # 1) MLP, Speed ì—†ìŒ
    main(model_type="mlp", use_speed=False)

    # 2) MLP, Speed ìˆìŒ
    # main(model_type="mlp", use_speed=True)

    # 3) LSTM, Speed ì—†ìŒ
    # main(model_type="lstm", use_speed=False)

    # 4) LSTM, Speed ìˆìŒ
    # main(model_type="lstm", use_speed=True)
