# stgcn_experiment.py (multi-task + ÌäúÎãù/Residual Î≤ÑÏ†Ñ)
#
# Î™©Ï†Å:
#   - X_samples.npy, Y_samples.npy, adjacency_norm.npy Î•º ÏÇ¨Ïö©Ìïú
#     "Î©ÄÌã∞ÌÉúÏä§ÌÅ¨ ST-GCN" Î™®Îç∏ ÌïôÏäµ/ÌèâÍ∞Ä.
#   - Ï∂úÎ†•: TotalTraffic(t+1), Speed(t+1) ÎèôÏãú ÏòàÏ∏°
#
# Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ (Ïù¥Ï†Ñ Î≤ÑÏ†Ñ ÎåÄÎπÑ):
#   - learning rate Í∏∞Î≥∏Í∞í: 1e-3 ‚Üí 5e-4 (configÏóêÏÑú ÏâΩÍ≤å ÏàòÏ†ï Í∞ÄÎä•)
#   - hidden_channels Í∏∞Î≥∏Í∞í: 64 ‚Üí 32 (configÏóêÏÑú ÏâΩÍ≤å ÏàòÏ†ï Í∞ÄÎä•)
#   - num_blocks Í∏∞Î≥∏Í∞í: 2 ‚Üí 1 (over-smoothing ÏôÑÌôî Î™©Ï†Å)
#   - STGCNBlock Ïóê residual connection Ï∂îÍ∞Ä
#       - in_channels != out_channels Ïù∏ Í≤ΩÏö∞ 1x1 conv Î°ú projection
#   - block ÎÇ¥Î∂ÄÏóê dropout ÏïΩÍ∞Ñ Ï∂îÍ∞Ä (Í≥ºÏ†ÅÌï©/Ìè≠Ï£º ÏôÑÌôî)

from pathlib import Path
from typing import Tuple

import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, Subset


# =========================
# 0. Í≥µÌÜµ ÏÑ§Ï†ï + ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
# =========================

data_dir = Path("/mnt/c/Source/python/AST-GCN/res")

X_path = data_dir / "X_samples.npy"      # (S, N, T_in, F)
Y_path = data_dir / "Y_samples.npy"      # (S, N, T_out, 2)
A_path = data_dir / "adjacency_norm.npy" # (N, N)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# üîß ÌäúÎãùÌïòÍ∏∞ ÏâΩÍ≤å ÏúÑÏóê Î™®ÏïÑÎë†
HIDDEN_CHANNELS = 32       # 32 / 64 / 128 Îì± Î∞îÍøîÍ∞ÄÎ©¥ÏÑú Ïã§Ìóò
NUM_BLOCKS      = 1        # 1 or 2 (over-smoothing ÌîºÌïòÎ†§Î©¥ 1Î∂ÄÌÑ∞)
LR              = 5e-4     # 1e-3 ‚Üí 5e-4 / 1e-4 Îì± ÏãúÎèÑ
DROPOUT_P       = 0.1      # block ÎÇ¥Î∂Ä dropout ÎπÑÏú®


# =========================
# 1. Dataset / DataLoader
# =========================

class TrafficSamplesDataset(Dataset):
    """
    X_samples, Y_samples Î•º ÎûòÌïëÌïòÎäî Dataset.

    X: (S, N, T_in, F)
    Y: (S, N, T_out, 2)  ‚Üê [TotalTraffic, Speed]
    """

    def __init__(self, X: np.ndarray, Y: np.ndarray):
        assert X.shape[0] == Y.shape[0], "ÏÉòÌîå Í∞úÏàò(S)Í∞Ä ÏÑúÎ°ú Îã§Î¶Ñ"
        self.X = torch.from_numpy(X).float()  # (S, N, T_in, F)
        self.Y = torch.from_numpy(Y).float()  # (S, N, T_out, 2)

    def __len__(self) -> int:
        return self.X.shape[0]

    def __getitem__(self, idx: int):
        return self.X[idx], self.Y[idx]


def get_dataloaders(
    X_path: Path,
    Y_path: Path,
    batch_size: int = 2,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    seed: int = 42,
):
    X = np.load(X_path)  # (S, N, T_in, F)
    Y = np.load(Y_path)  # (S, N, T_out, 2)

    S, N, T_in, F = X.shape
    S2, N2, T_out, num_targets = Y.shape
    assert S == S2 and N == N2, "X, YÏùò ÏÉòÌîå Í∞úÏàò / ÎÖ∏Îìú ÏàòÍ∞Ä Îã§Î¶Ñ"
    assert num_targets == 2, "Y ÎßàÏßÄÎßâ Ï∂ïÏùÄ 2 (TotalTraffic, Speed) Ïù¥Ïñ¥Ïïº Ìï®"

    print(f"[Data] X: {X.shape}, Y: {Y.shape}")
    print(f"[Data] N={N}, T_in={T_in}, F={F}, T_out={T_out}, num_targets={num_targets}")

    dataset = TrafficSamplesDataset(X, Y)

    rng = np.random.RandomState(seed)
    indices = np.arange(S)
    rng.shuffle(indices)

    n_test = int(S * test_ratio)
    n_val = int(S * val_ratio)
    n_train = S - n_val - n_test

    train_idx = indices[:n_train]
    val_idx   = indices[n_train:n_train + n_val]
    test_idx  = indices[n_train + n_val:]

    print(f"[Split] train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}")

    train_ds = Subset(dataset, train_idx)
    val_ds   = Subset(dataset, val_idx)
    test_ds  = Subset(dataset, test_idx)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)

    meta = dict(N=N, T_in=T_in, F=F, T_out=T_out, num_targets=num_targets)
    return train_loader, val_loader, test_loader, meta


# =========================
# 2. ST-GCN Í¥ÄÎ†® Î™®Îìà (Residual + Dropout Ï∂îÍ∞Ä)
# =========================

class GraphConv(nn.Module):
    """
    (ÏïÑÏ£º Îã®ÏàúÌïú) Í∑∏ÎûòÌîÑ Ïª®Î≥ºÎ£®ÏÖò Î†àÏù¥Ïñ¥.

    ÏûÖÎ†•:
      x: (B, C_in, T, N)
      A: (N, N)

    ÎèôÏûë:
      1) Ï±ÑÎÑê Î∞©Ìñ• 1x1 Conv (theta) Î°ú ÌîºÏ≤ò Î≥ÄÌôò
      2) Ïù∏Ï†ëÌñâÎ†¨ A Î•º ÏÇ¨Ïö©Ìï¥ ÎÖ∏Îìú Î∞©Ìñ•ÏúºÎ°ú Î©îÏãúÏßÄ Ï†ÑÎã¨:
         y[b, c, t, i] = sum_j A[i, j] * x_theta[b, c, t, j]
    """

    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.theta = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=(1, 1),
        )

    def forward(self, x: torch.Tensor, A: torch.Tensor) -> torch.Tensor:
        x = self.theta(x)  # (B, C_out, T, N)
        x = torch.einsum("ij, bctj -> bcti", A, x)  # (B, C_out, T, N)
        return x


class STGCNBlock(nn.Module):
    """
    ÌïòÎÇòÏùò ST-GCN Î∏îÎ°ù (Residual + Dropout Ìè¨Ìï® Î≤ÑÏ†Ñ).

    Íµ¨Ï°∞:
      input x -> TemporalConv1 -> ReLU
              -> GraphConv     -> ReLU
              -> TemporalConv2
              -> (Residual Add) -> ReLU
              -> Dropout

    - in_channels != out_channels Ïù∏ Í≤ΩÏö∞,
      residual Ïó∞Í≤∞ Ï†ÑÏóê 1x1 Conv Î°ú projection ÏßÑÌñâ.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        use_residual: bool = True,
        dropout: float = 0.1,
    ):
        super().__init__()
        padding = kernel_size // 2

        self.use_residual = use_residual
        self.dropout_p = dropout

        # ÏãúÍ∞ÑÏ∂ïÏóêÎßå conv
        self.temporal1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=(kernel_size, 1),
            padding=(padding, 0),
        )

        self.graph_conv = GraphConv(
            in_channels=out_channels,
            out_channels=out_channels,
        )

        self.temporal2 = nn.Conv2d(
            in_channels=out_channels,
            out_channels=out_channels,
            kernel_size=(kernel_size, 1),
            padding=(padding, 0),
        )

        # residual projection (Ï±ÑÎÑê ÏàòÍ∞Ä Îã§Î•º ÎïåÎßå ÏÇ¨Ïö©)
        if use_residual and in_channels != out_channels:
            self.res_proj = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=(1, 1),
            )
        else:
            self.res_proj = None

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor, A: torch.Tensor) -> torch.Tensor:
        """
        x: (B, C_in, T, N)
        A: (N, N)
        return: (B, C_out, T, N)
        """
        identity = x

        # 1) Temporal Conv -> ReLU
        out = self.temporal1(x)
        out = self.relu(out)

        # 2) Graph Conv -> ReLU
        out = self.graph_conv(out, A)
        out = self.relu(out)

        # 3) Îã§Ïãú Temporal Conv
        out = self.temporal2(out)

        # 4) Residual Ïó∞Í≤∞
        if self.use_residual:
            if self.res_proj is not None:
                identity = self.res_proj(identity)
            out = out + identity

        # 5) ReLU + Dropout
        out = self.relu(out)
        out = self.dropout(out)

        return out


class STGCNMultiTask(nn.Module):
    """
    Ï†ÑÏ≤¥ ST-GCN Î©ÄÌã∞ÌÉúÏä§ÌÅ¨ Î™®Îç∏.

    ÏûÖÎ†•:
      x: (B, N, T_in, F)

    Ï∂úÎ†•:
      y_hat: (B, N, T_out, num_targets=2)
    """

    def __init__(
        self,
        N_nodes: int,
        T_in: int,
        F_in: int,
        T_out: int,
        num_targets: int,
        A_norm: np.ndarray,
        hidden_channels: int = 32,
        num_blocks: int = 1,
        kernel_size: int = 3,
        dropout: float = 0.1,
        use_residual: bool = True,
    ):
        super().__init__()
        self.N_nodes = N_nodes
        self.T_in = T_in
        self.F_in = F_in
        self.T_out = T_out
        self.num_targets = num_targets

        A = torch.tensor(A_norm, dtype=torch.float32)
        self.register_buffer("A", A)

        blocks = []
        in_c = F_in
        for b in range(num_blocks):
            block = STGCNBlock(
                in_channels=in_c,
                out_channels=hidden_channels,
                kernel_size=kernel_size,
                use_residual=use_residual,
                dropout=dropout,
            )
            blocks.append(block)
            in_c = hidden_channels

        self.blocks = nn.ModuleList(blocks)

        # ÎßàÏßÄÎßâ hidden feature(C) ‚Üí T_out * num_targets
        self.fc_out = nn.Linear(hidden_channels, T_out * num_targets)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, N, T_in, F) ‚Üí y: (B, N, T_out, num_targets)
        """
        B, N, T_in, F = x.shape
        assert N == self.N_nodes, "ÎÖ∏Îìú Ïàò NÏù¥ Ïù∏Ï†ëÌñâÎ†¨Í≥º ÎßûÏßÄ ÏïäÏäµÎãàÎã§"
        assert T_in == self.T_in and F == self.F_in

        # (B, N, T, F) -> (B, F, T, N)
        x = x.permute(0, 3, 2, 1)  # (B, F, T_in, N)

        for block in self.blocks:
            x = block(x, self.A)  # (B, hidden, T_in, N)

        # ÎßàÏßÄÎßâ ÏãúÍ∞Ñ Ïä§ÌÖùÎßå ÏÇ¨Ïö©
        h_last = x[:, :, -1, :]    # (B, hidden, N)
        h_last = h_last.permute(0, 2, 1)  # (B, N, hidden)

        y_flat = self.fc_out(h_last)  # (B, N, T_out * num_targets)
        y = y_flat.view(B, N, self.T_out, self.num_targets)
        return y


# =========================
# 3. ÌïôÏäµ / ÌèâÍ∞Ä Î£®ÌîÑ
# =========================

def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module,
) -> Tuple[float, float, float, float]:
    model.train()
    total_loss = 0.0
    total_mae_all = 0.0
    total_mae_tr = 0.0
    total_mae_sp = 0.0
    total_count = 0

    for xb, yb in loader:
        xb = xb.to(device)
        yb = yb.to(device)

        optimizer.zero_grad()

        y_hat = model(xb)
        loss = criterion(y_hat, yb)

        loss.backward()
        optimizer.step()

        B = xb.size(0)
        total_loss += loss.item() * B

        diff = (y_hat - yb).abs()
        mae_all = diff.mean().item()
        mae_tr = diff[..., 0].mean().item()
        mae_sp = diff[..., 1].mean().item()

        total_mae_all += mae_all * B
        total_mae_tr += mae_tr * B
        total_mae_sp += mae_sp * B
        total_count += B

    avg_loss = total_loss / total_count
    avg_mae_all = total_mae_all / total_count
    avg_mae_tr = total_mae_tr / total_count
    avg_mae_sp = total_mae_sp / total_count
    return avg_loss, avg_mae_all, avg_mae_tr, avg_mae_sp


@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
) -> Tuple[float, float, float, float]:
    model.eval()
    total_loss = 0.0
    total_mae_all = 0.0
    total_mae_tr = 0.0
    total_mae_sp = 0.0
    total_count = 0

    for xb, yb in loader:
        xb = xb.to(device)
        yb = yb.to(device)

        y_hat = model(xb)
        loss = criterion(y_hat, yb)

        B = xb.size(0)
        total_loss += loss.item() * B

        diff = (y_hat - yb).abs()
        mae_all = diff.mean().item()
        mae_tr = diff[..., 0].mean().item()
        mae_sp = diff[..., 1].mean().item()

        total_mae_all += mae_all * B
        total_mae_tr += mae_tr * B
        total_mae_sp += mae_sp * B
        total_count += B

    avg_loss = total_loss / total_count
    avg_mae_all = total_mae_all / total_count
    avg_mae_tr = total_mae_tr / total_count
    avg_mae_sp = total_mae_sp / total_count
    return avg_loss, avg_mae_all, avg_mae_tr, avg_mae_sp


# =========================
# 4. Î©îÏù∏ Ïã§Ìñâ
# =========================

def main():
    train_loader, val_loader, test_loader, meta = get_dataloaders(
        X_path=X_path,
        Y_path=Y_path,
        batch_size=2,
        val_ratio=0.15,
        test_ratio=0.15,
        seed=42,
    )
    N = meta["N"]
    T_in = meta["T_in"]
    F = meta["F"]
    T_out = meta["T_out"]
    num_targets = meta["num_targets"]
    print("Meta:", meta)

    A_norm = np.load(A_path)  # (N, N)
    assert A_norm.shape == (N, N), "A_norm shapeÏù¥ XÏùò NÍ≥º Îã§Î¶ÖÎãàÎã§"
    print("[Adjacency] Loaded A_norm:", A_norm.shape)

    model = STGCNMultiTask(
        N_nodes=N,
        T_in=T_in,
        F_in=F,
        T_out=T_out,
        num_targets=num_targets,
        A_norm=A_norm,
        hidden_channels=HIDDEN_CHANNELS,
        num_blocks=NUM_BLOCKS,
        kernel_size=3,
        dropout=DROPOUT_P,
        use_residual=True,
    ).to(device)

    print(model)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=LR,
        weight_decay=1e-5,
    )

    num_epochs = 100
    best_val_loss = float("inf")
    best_state = None

    for epoch in range(1, num_epochs + 1):
        train_loss, train_mae_all, train_mae_tr, train_mae_sp = train_one_epoch(
            model, train_loader, optimizer, criterion
        )
        val_loss, val_mae_all, val_mae_tr, val_mae_sp = evaluate(
            model, val_loader, criterion
        )

        print(
            f"[Epoch {epoch:03d}] "
            f"Train Loss: {train_loss:.4f}, MAE(all): {train_mae_all:.4f}, "
            f"MAE(traffic): {train_mae_tr:.4f}, MAE(speed): {train_mae_sp:.4f} | "
            f"Val Loss: {val_loss:.4f}, MAE(all): {val_mae_all:.4f}, "
            f"MAE(traffic): {val_mae_tr:.4f}, MAE(speed): {val_mae_sp:.4f}"
        )

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = model.state_dict()

    if best_state is not None:
        model.load_state_dict(best_state)

    test_loss, test_mae_all, test_mae_tr, test_mae_sp = evaluate(
        model, test_loader, criterion
    )
    print(
        f"[Test] Loss: {test_loss:.4f}, MAE(all): {test_mae_all:.4f}, "
        f"MAE(traffic): {test_mae_tr:.4f}, MAE(speed): {test_mae_sp:.4f}"
    )

    save_path = data_dir / "stgcn_multitask_tuned_best.pth"
    torch.save(
        {
            "model_type": "stgcn_multitask_tuned",
            "state_dict": model.state_dict(),
            "meta": meta,
            "config": {
                "hidden_channels": HIDDEN_CHANNELS,
                "num_blocks": NUM_BLOCKS,
                "lr": LR,
                "dropout_p": DROPOUT_P,
            },
        },
        save_path,
    )
    print("Saved best model to:", save_path)


if __name__ == "__main__":
    main()
